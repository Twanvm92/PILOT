# PILOT

PILOT is a technical debt detector built on top of a combination of different natural language processing (NLP) and machine learning (ML) techniques.

## Generate the feature matrices
- Run the Python script features-matrices-dataset-debthunter.py to generate the features-matrices related to debthunter dataset
- Run the Python script features-matrices-dataset-maldonato.py to generate the features-matrices related to Maldonado dataset

## Run the classifier engines

import Runner as runner
runner.run('DatasetD3/Round1/', [4003, 10, 2], 100, 3.0, 0.75)

runner.displayCM('DatasetD2/',5)
runner.displayCM('DatasetD3/',2)

runner.runExperiment('DatasetD3/', [4003, 10, 2], 100, 3.0, 0.75)
runner.displayROC('DatasetD3/',2)


runner.runExperiment('DatasetD1/', [5072, 10, 5], 100, 3.0, 0.75)
runner.run('DatasetD1/Round1/', [5072, 10, 5], 100, 3.0, 0.75)
runner.displayCM('DatasetD1/',5)
runner.displayROC('DatasetD1/',5)

<!-- create header and bulletlists in markdown -->
# Comments
- I think 10-fold split is done on all data instead of 9 projects vs 1 project.
- feature matrice is generated by the feature generator py files in root and they take as input
the datasets from ./datasets of which maldonado dataset is the original and of which the debthunter one is an already subsampled and nlp-processed file (partially) from Debthunter paper (which is also in the Maldonado repo under datasets training.csv (all data)). Might explain why the tool performs better than expected? its basically within + cross project training depending on how the data was partitioned.
    - Can assume probably same was done for Debthunter evaluation.
- datasets in ./datasets-input-neural-network seem to be 10 fold split (don't know if random? => can check) from the **binary** feature matrices that are generated by the feature matrice files because only the binary feature matrice has the same size as the debthunter-dataset (or maldonado if maldonado feature matrices were generated) while the **multi-class** feature matrices are way smaller. This is because multi-class classification is only done on comments that are labeled SATD in the first classification step. So for getting features used for training for multi-class, only the comments that are labeled anything but "Without Classification" in the csv are used (hence the like 2.2k length csv file after doing multiclass feature matrice generation on debthunter dataset). 
    - Can do this 10 fold repeating with the Maldonado dataset?
- for the network layer size input array:
    - first element is the size of the feature vector (#columns in feature matrice)
        - the README here shows 2 examples: 4003 for D2 dataset Binary classification case and 5072 for the D1 multi-class classification case
            - In the PILOT paper see Table 2. here we see 4004 and 5073 instead.
    - second element is the middle hidden layer with 10 neurons. Paper experiments with 1 to two of these layers with 10 neurons.
    - last element is the output: 2 for binary case and 5 for multi-class.
- For D2 dataset the labels are only already labeled for binary case. That's why training + test data per round is always the total amount of data used.
    - for reproducing results Figure 3a we can use the data that is giving to us in DatasetD2
    - for reproducing results Figure 3b we would have to preprocess ./datasets/debthunter-dataset.csv ourselves to get label files for training and test like how it is done for D1 already.
- For D1 dataset the labels are only already labeled for multi-class case. That's why training + test data per round is always only the total amount of comments used that were manually labeled as WITHOUT_CLASSIFICATION.
    - For replication of original paper results we only need this setting anyways (RQ2)
- It looks like in the paper for every round they appended the groundtruth and prediction csvs because each comment has been in the test data once in a round. Then in the end there is one groundtruth and prediction csv and then they use the functions they made to make the confusion matrices. **one of these files per classification case (binary/multi-class) per dataset used (D1/D2)!**
    - need to calculate precision, f1 and recall from these files.



